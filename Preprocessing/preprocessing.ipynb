{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This preprocessing file should be utilized to do normalizations, feature extraction, etc.\n",
    "Files created from this preprocessing are the files that should be used for merging (joining with the egg price).\n",
    "\n",
    "### Naming\n",
    "- Name the file \"...._for_merge.csv\"\n",
    "- Use Path (to data folder) to store the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where we store our preprocessed data\n",
    "data_file_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_disasters = pd.read_csv(f\"{data_file_path}/date_organized_us_disasters.csv\", index_col=0) # Load date preprocessed data.\n",
    "df_us_diseases = pd.read_csv(f\"{data_file_path}/date_organized_us_diseases.csv\", index_col=0)\n",
    "df_us_population = pd.read_csv(f\"{data_file_path}/date_organized_us_population.csv\",index_col=0)\n",
    "df_us_covid = pd.read_csv(f\"{data_file_path}/date_organized_us_covid.csv\", index_col=0)\n",
    "df_us_weather = pd.read_csv(f\"{data_file_path}/date_organized_weather_agg.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Natural Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts how many months a disaster lasted, distributes total cost and deaths across those months.\n",
    "df_us_disasters['event_months'] = df_us_disasters.groupby(['name', 'disaster_type'])['date'].transform('count')\n",
    "df_us_disasters['adjusted_cpi_cost'] = df_us_disasters['cpi_adjusted_cost'] / df_us_disasters['event_months']\n",
    "df_us_disasters['adjusted_unadjusted_cost'] = df_us_disasters['unadjusted_cost'] / df_us_disasters['event_months']\n",
    "df_us_disasters['adjusted_deaths'] = df_us_disasters['deaths'] / df_us_disasters['event_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_disasters = df_us_disasters.groupby('date')[['adjusted_cpi_cost', 'adjusted_unadjusted_cost', 'adjusted_deaths']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_disasters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation to normalize cost values\n",
    "df_us_disasters['log_cpi_adjusted_cost'] = np.log1p(df_us_disasters['adjusted_cpi_cost'])  # log(1+x) to avoid log(0)\n",
    "df_us_disasters['log_unadjusted_cost'] = np.log1p(df_us_disasters['adjusted_unadjusted_cost'])\n",
    "\n",
    "df_us_disasters = df_us_disasters.drop(['adjusted_cpi_cost', 'adjusted_unadjusted_cost'], axis=1)\n",
    "df_us_disasters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this data to join \n",
    "df_us_disasters.to_csv(f'{data_file_path}/df_us_disasters_for_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Avian Flu in Birds and Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird = pd.read_csv(\"../data/date_organized_avian_flu_bird.csv\", index_col = 0)\n",
    "human = pd.read_csv(\"../data/date_organized_avian_flu_human.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flocks_impacted = bird.groupby('yyyy_mm').size().reset_index(name = 'Flock_Count')\n",
    "flocks_impacted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird = bird.merge(flocks_impacted, on = 'yyyy_mm', how = 'left')\n",
    "bird.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flocks_impacted_size = bird.groupby('yyyy_mm')['Flock Size'].sum().reset_index(name = 'Total_Flock_Size')\n",
    "bird = bird.merge(flocks_impacted_size, on = 'yyyy_mm', how = 'left')\n",
    "bird.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log normalize the Total_Flock_Size column since the values are skewed in the millions. (e.g. 10mi, 25mil, 35mil, 300k, 400k, <100k are the common values)\n",
    "bird['Total_Flock_Size'] = bird['Total_Flock_Size'].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "bird.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_impacted = human.groupby('yyyy_mm')['Human cases with highly pathogenic avian influenza A/H5N1 (monthly)'].sum().reset_index(name = 'People_Count')\n",
    "people_impacted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird.to_csv('../data/df_avian_flu_bird_for_merge.csv')\n",
    "people_impacted.to_csv('../data/df_avian_flu_human_for_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized disease outbreak by us population\n",
    "df_us_diseases = df_us_diseases[['date', 'us_human_outbreaks_cnt', 'us_human_illnesses_cnt']].copy()\n",
    "df_us_diseases = df_us_diseases.merge(df_us_population, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_diseases['outbreaks_per_million'] = df_us_diseases['us_human_outbreaks_cnt']/df_us_diseases['population_million']\n",
    "df_us_diseases['illnesses_per_million'] = df_us_diseases['us_human_illnesses_cnt']/df_us_diseases['population_million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_diseases_for_merge = df_us_diseases[['date', 'outbreaks_per_million', 'illnesses_per_million']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_diseases_for_merge.to_csv(f'{data_file_path}/df_us_diseases_for_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for covid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_covid = df_us_covid.merge(df_us_population, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_covid['hospitalized_per_million'] = df_us_covid['avg_daily_hospitalized']/df_us_covid['population_million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_covid_for_merge = df_us_covid[['date', 'hospitalized_per_million']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_covid_for_merge.to_csv(f'{data_file_path}/df_us_covid_for_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_us_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging the temperatures in the big egg-producing states for an overall temperature value.\n",
    "df_us_weather['temp_overall'] = df_us_weather[['temp_IN', 'temp_MO', 'temp_WI']].mean(axis=1)\n",
    "\n",
    "weather_summer = pd.concat([df_us_weather[df_us_weather['date'].str[5:] == '06'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '07'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '08']])\n",
    "weather_summer.sort_index(inplace=True) \n",
    "\n",
    "weather_fall = pd.concat([df_us_weather[df_us_weather['date'].str[5:] == '09'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '10'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '11']])\n",
    "weather_fall.sort_index(inplace=True) \n",
    "\n",
    "weather_winter = pd.concat([df_us_weather[df_us_weather['date'].str[5:] == '12'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '01'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '02']])\n",
    "weather_winter.sort_index(inplace=True) \n",
    "\n",
    "weather_spring = pd.concat([df_us_weather[df_us_weather['date'].str[5:] == '03'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '04'],\n",
    "                           df_us_weather[df_us_weather['date'].str[5:] == '05']])\n",
    "weather_spring.sort_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_spring['temp_variance'] = abs(weather_spring['temp_overall'] - weather_spring['temp_overall'].mean())\n",
    "weather_summer['temp_variance'] = abs(weather_summer['temp_overall'] - weather_summer['temp_overall'].mean())\n",
    "weather_fall['temp_variance'] = abs(weather_fall['temp_overall'] - weather_fall['temp_overall'].mean())\n",
    "weather_winter['temp_variance'] = abs(weather_winter['temp_overall'] - weather_winter['temp_overall'].mean())\n",
    "\n",
    "print(weather_winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_over = pd.concat([weather_winter, weather_spring, weather_summer, weather_fall]).sort_values('date')\n",
    "print(weather_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_weather_for_merge = weather_over[['date', 'temp_overall', 'temp_variance']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_weather_for_merge.to_csv(f'{data_file_path}/df_us_weather_for_merge.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
